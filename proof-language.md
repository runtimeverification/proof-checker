---
geometry: margin=2cm
---

\newcommand{\limplies}{\to}
\renewcommand{\phi}{\varphi}

Motivation
==========

K's behaviour has its mathematical foundation matching logic. Each of its
constructs may be reduced to simpler and more fundamental concepts in matching
logic. Each of the K tools such as `krun`, `kprove`, etc. can, *in theory*,
explain themselves in terms of matching logic. Our goal is to change that theory
to practice. We want to instrument each of these tools with enough information
to generate formal matching logic proofs.

There are two things we need to make this work. First, we need a *proof
checker*---a program that can takes as input a matching logic proof and checks
that it is correct. We need this program to be small and simple, so we can
easily verify it (at first by manual inspection, and eventually through formal
verification). It also needs to be fast and efficient to be practical, since
proofs can be hundreds of thousands of steps long.

The second is a proof generator. It will take the instrumentation generated by
the backends, and produce a matching logic proof. In [previous work], we wrote
such a generator for a logic-agnostic proof checker called Metamath.

[previous work]: https://xchen.page/assets/pdf/LCT+23-paper.pdf "Generating Proof Certificates for a Language-Agnostic Deductive Program Verifier"

Unfortunately, we have realized that the Metamath does not meet our needs. Since
it is not matching logic specific, it is very verbose. It is intended to be
somewhat human readable (though it is debatable whether it achieves this) at the
cost of efficiency. It's also a pretty complex language. This has made matching
logic proofs using it massive (we've seen some that are gigabytes), and
impractical.

This has motivated us to define our own compact binary format,
tailored to matching logic. It's essence is a giant postfix/reverse polish
notation representation of a matching logic proof as a DAG. We think that this
is will allow us to greatly speed up proof checking.


FAQ
===

>   Is the huge size of the proofs only due to using metamath?
>
>   My understanding was that it's also due to the (too) small axiomatization
>   used for ML which requires some results (e.g., such that one would prove
>   generically by induction of the size of the proof term) to be instead
>   reproved in each concrete instance they are used.
>
>   I know that at some point Brandon implemented a prover directly on top
>   of the haskell backend, and managed to produce a proof of ~2GB showing
>   something along the lines of 1 + 1 = 2

I'm not sure what Brandon's formalization looked like exactly. It's possible
that both the Metamath and our new proposed format are at slightly higher level
than Brandon's.

While we're trying to keep things as low level as we can while still being
practical---so we allow things like metavariables. Meta-variables let us avoid
re-proving theorems for each concrete instance---we can instantiate the
meta-variables with concrete patterns, and reuse the proofs.

In my experience with proof generation in Metamath and Metamath
Zero (another checker), a huge part of proof size is carefully choosing the
appropriate lemmas.

With metamath this is even more tricky, because it is expensive even to restate
a lemma when pattern/configuration sizes are big. The new format addresses
this by allowing sharing of common subpatterns by essentially treating proofs
and patterns uniformly---we may share subpatterns the same way lemmas allow us
to share subproofs.

The second problem we had with metamath is that a majority
of the proof become proving the well-formedness of patterns over and over again.
Since there is a simple decision procedure to check this, there are huge space
savings from directly implementing this in the checker.


> ... I thought that the advantage of using Metamath would be that it would be easier to trust.

The Metamath checker itself is easy to trust because it has a large community behind it, that has been extensively vetted it.
But, besides the checker, we must also trust the specifications and theories written in Metamath.
It is established that Metamath makes it easy to write incorrect specifications.

1.  Notation is typically defined as trusted axioms in Metamath.
    Incorrectly or unsound notation would allow us to prove all sorts of things,
    when we were only expecting a conservative extension.

2.  Worse, the syntax of the logic is defined as axioms. This makes it easy to
    define an ambiguous grammar, especially in the presence of notation. Note
    that metamath doesn't have a native concept of terms, but works over strings
    of tokens. So it is even possible for unbalanced parenthesis to be a "formula".

[set.mm] and other Metamath repositories dealt with this with external tools
to make sure notation and grammar are well-formed. But, these tools are now part
of our trust base?
We will need all sorts of well-formedness checkers for metamath+ML specifications
people write.

[set.mm]: https://github.com/metamath/set.mm/blob/develop/set.mm

> Why is proof generation done in Python, while the proof checker in Rust?

The proof generation code will need to integrate
with the K pyk library. Besides, we care more about ease of writing proofs, than
efficiency or verifiability as in the proof-checker.

These goals directly contradict the goals of the checker since it needs to be
streamlined, both for efficiency and ease of verification---we even avoid basic
debugging and pretty printing to save on complexity. So, even if we wrote
the proof generation code in rust, it would make sense to not reuse the
checkers code.


Matching logic proof format
===========================

This is a proposed matching logic proof format.

## Relation to Metamath

Note that this language heavily based on the metamath format for compressed proofs.
Besides being a binary rather than ASCII encoding, there are two major changes.

1.  We use the same mechanism for constructing terms and proofs, allowing better
    compression.
2.  We allow emmitting multiple proofs from the same representation. This allows
    us to represent lemmas and compression uniformly.
    That is, we can store the entire contents of a single metamath file in this format,
    rather than just the proof of a single lemma.

## Relation to postfix notation/RPN

The format may be viewed as a generalization
of postfix notation (incidentally also called Lukaseiwicz notation),
for an expression computing the conclusion of a proof.

We also allow marking subexpressions (i.e.Â either proofs or patterns) for reuse. This
allows us to represent lemmas, and compactly representing configurations and notation.

(This needs input from ZK folk.) Finally, we allow marking constructed proofs
as public. This serves two purposes. First, it allows us to publish lemmas
that may be reused in other proofs. Second, it allows us to publish the main
result proved by the proof.


## As a stack machine

It may also be viewed language interpreted using a stack machine.
This stack machines constructs terms---either patterns or proofs. We treat
patterns and proofs uniformly---at an abstract level, they are both terms we
must construct, with particular well-formedness criteria. It is intended for the
most part, that **the verifier only needs to keep proved conclusions in
memory**.


## Goals:

-   No upfront parsing cost.
-   Easily verifiable using ZK.
-   Low memory usage.
-   Composability.


## Non-goals:

-   Parallization: I have not been thinking explicitly about parallel verification of this format,
    because I expect parallelization to happen by composing units of this format into larger proofs.
    Even so I think with some additional metadata, we could make
    this format parallelizable.

-   Human readability: We aim to be a simple binary format. To become human
    readable, we expect the user to run a secondary program to pretty-print the
    proof. Note that symbols, lemmas, and notation do not have names.


Terms
=====

As mentioned previously, we may view this format as a language for constructing
terms that meet particular well-formedness criteria. Let us first enumerate
those terms, in a python-esque pseudocode.

`Term`s are of two types, `Pattern`, and `Proof`.
Every `Term` has a well-formedness check.
Every `Proof` has a conclusion.

```python
abstract class Term:
    abstract def well_formed:
        ...

abstract class Pattern(Term):
    def well_formed():
        # Fails if any two MetaVars with the same id has
        # different meta-requirements
        # MetaVars are introduced later.
    ...

    def e_fresh(evar):
        # returns true iff evar is not a free element variable in this pattern
    ...

    def s_fresh(svar):
        # returns true iff svar is not a free set variable in this pattern
    ...

abstract class Proof(Term):
    abstract def conclusion:
        ...
```

`Pattern`s
----------

There are two classes of `Pattern`,
the expected concrete constructors for matching logic patterns,
and a representation for various "meta" patterns.

### Standard patterns

```python
class Symbol(Pattern):
    name: u32

    def e_fresh(evar):
        return True

    def s_fresh(svar):
        return True

class SVar(Pattern):
    name: u32

    def e_fresh(evar):
        return True

    def s_fresh(svar):
        return svar.name != name

class EVar(Pattern):
    name: u32

    def e_fresh(evar):
        return evar.name != name

    def s_fresh(svar):
        return True

class Implication(Pattern):
    left: Pattern
    right: Pattern

    def e_fresh(evar):
        return left.e_fresh(evar) and right.e_fresh(evar)

    def s_fresh(svar):
        return left.s_fresh(svar) and right.s_fresh(svar)

class Application(Pattern):
    left: Pattern
    right: Pattern

    def e_fresh(evar):
        return left.e_fresh(evar) and right.e_fresh(evar)

    def s_fresh(svar):
        return left.s_fresh(svar) and right.s_fresh(svar)

class Exists(Pattern):
    var: EVar
    subpattern: Pattern

    def e_fresh(evar):
        return evar == var or subpattern.e_fresh(evar)

    def s_fresh(svar):
        return subpattern.s_fresh(svar)

class Mu(Pattern):
    var: SVar
    subpattern: Pattern

    def e_fresh(evar):
        return subpattern.e_fresh(evar)

    def s_fresh(svar):
        return var == svar or subpattern.s_fresh(svar)

    def well_formed():
        return super.well_formed()
           and subpattern.var_occurs_positively(var)
```

### Meta-patterns

Meta-patterns allow us to represent axiom- and theorem-schemas through the use
of metavariables.
Each `MetaVar` has a list of constraints that must be met by any instantiation.
These may also be used by the well-formedness checks for `Proof`s.
Since we only know constraints of a meta-pattern, constraint checking is best-effort.
This means we want to reject anything that we can't prove to be satisfy the constraints.
For well-formedness, we use the same meta-reasoning as the one used by the MM formalization
(denoted with the comments):

```python
class MetaVar(Pattern):
    name: u32

    # Meta-requirements that must be satisfied by any instantiation.
    e_fresh: set[u32]             # Element variables that must not occur in an instatiation
    s_fresh: set[u32]             # Set variables that must not occur in an instatiation
    positive: set[u32]            # Set variables that must only occur positively in an instatiation
    negative: set[u32]            # Set variables that must only occur negatively in an instatiation
    application_context: set[u32] # Element variables that must only occur as a hole variable in an application context.

    def e_fresh(evar):
        return evar in this.e_fresh

    def s_fresh(svar):
        return svar in this.s_fresh

    def well_formed():
        return super.well_formed()
           and s_fresh.issubset(positive) # positive-disjoint
           and s_fresh.issubset(negative) # negative-disjoint
```

We also need to represent substitutions applied to `MetaVar`s.

```python
# TODO: Might actually consider making ESubst really only pattern
# and compute e_fresh, s_fresh, etc. from the given pattern+var+plug comb
class ESubst(MetaVar):
    pattern: MetaVar
    var: EVar
    plug: Pattern

# TODO: Might actually consider making ESubst really only pattern
# and compute e_fresh, s_fresh, etc. from the given pattern+var+plug comb
class SSubst(MetaVar):
    pattern: MetaVar
    var: SVar
    plug: Pattern

    def well_formed():
        if        var == plug                      # subst-id
            or    var in pattern.s_fresh           # subst-fresh
            return pattern.well_formed()

        if plug.s_fresh(var) and not this.s_fresh(X):
            return false                           # fresh-in-subst

        for X in pattern.s_fresh:
            if plug.s_fresh(X) and not this.s_fresh(X):
                return false                       # fresh-after-subst

        if is_instance(pattern, SSubst)
            and var in pattern.pattern.s_fresh
            and pattern.plug == var:
                return pattern.pattern.well_formed # subst-inverse

        # TODO: subst-fold (subst-unfold not needed, as we really only want
        # to use the reasoning to simplify)

        return super.well_formed()
```


`Proof`s
--------

### Axiom Schemas

Axiom schemas are `Proof`s that do not need any input arguments.
They may use `MetaVar`s to represent their schematic nature.

```python
class Lukaseiwicz(Proof):
    def conclusion():
        phi1 = MetaVar('phi1')
        return Implication(Implication(Implication(MetaVar(phi1) , ...)...)...)

class Quantifier(Proof):
    def conclusion():
        x = EVar('#x')
        y = EVar('#y')
        phi = MetaVar('phi', fresh=[y])
        return Implication(ESubst(phi, x, y), Exists(x, phi))

class PropagationOr(Proof):
    def conclusion():
        hole = EVar('#hole')
        C = MetaVar(application_context=(EVar('#hole'),))
        phi1 = MetaVar('#phi1')
        phi2 = MetaVar('#phi2')
        return Implication(ESubst(C, or(phi1, phi2), hole), or(ESubst(C, phi1, hole), ESubst(C, phi2, hole)))

...
```


### Meta Variable Instantiation

Using a rule of meta inference, we may instantiate metatheorems.
This allows us to instantiate axiom and theorem schemas, such as
$\phi \limplies \phi$.


```python
class InstantiateSchema(Proof):
    subproof : Proof
    metavar: MetaVar
    instantiation: Pattern

    def well_formed():
        # Fails if the instantiation does not meet the
        # disjoint/positive/freshness/application ctx
        # criterea of the metavar.

    def conclusion():
        return subproof.meta_substitute(metavar, instantiation)
```

TODO: Could we just merge this with `InstantiateSchema`?
We may also use metavariables to represent notation.

```python
class InstantiateNotation(Pattern):
    notation: Pattern
    metavar_id: u32
    instantiation: Pattern

    def well_formed():
        # Fails if the instantiation does not meet the
        # disjoint/positive/freshness/application ctx
        # criterea of the metavar.

    def conclusion():
        return notation.meta_substitute(metavar, instantiation)
```


### Ordinary inference

```python
class ModusPonens(Proof):
    premise_left: Implication
    premise_right: Pattern

    def conclusion():
        return premise_left.right

    def well_formed():
        assert premise_right == premise_left.left

class Generalization(Proof):
    premise: Implication

    def phi1():
        premise.left
    def phi2():
        premise.right

    def conclusion():
        return Implication(ESubst(MetaVar(phi), EVar(x), EVar(y)), Exists(EVar(x), MetaVar(phi)))

    def well_formed():
        assert EVar(x) is fresh in phi1()
...
```

Verification
============

The verifier operates in three phases---the `gamma` phase, the `claim` phase, and the `proof` phase.
The `gamma` phase sets up the axioms that may be used in the proof.
The `claim` phase sets up claims, or theorems expected to be proved.
These two phases consume input from a "public" file, in ZK terminology.
The third phase, `proof`, proves each of these claims using the axioms.
The input to this phase is "private" in ZK terminology, allowing up us to roll-up the proof.
The semantics of instructions in each of these phases is identical, except
for the publish instruction. We will expand on this later.

The verifier's state consists of:

*   a *stack* of terms (i.e. lists of ids, patterns and proved conclusions).
*   a *memory* consisting of previously constructed patterns and proven conclusions that may be reused for later proofs.
*   only during the phase `gamma`, a write-only list of axioms.
    At the beginning of the `proof` phase this list of axioms will be used to pre-populate the memory.
*   only during the phases `claim` and `proof`: a queue of claims to be proved.

The verifier's input consists of three files, one for each phase.
These files are at a logical level. Depending on the circumstances
the implementation may choose to represent them as three distinct OS files,
or a single input stream separated by the `EOF` instruction.
In the case of Risc0, we will likely combine the first two files, since they are public
and keep the third separate, since it is private.

Between phases, the stack and memory are cleared.
Before the the start of the second phase,
the list of axioms populated in the first phase is used to initialize the memory.

> TODO: We need to think about this.
> I'd like notation to be shared between the phases,
> but don't want arbitary intermediate terms saved to the memory to be shared.
> Should we have an additional instruction for publishing notation?

Instructions and semantics
==========================

Each of these instructions checks that the constructed `Term` is well-formed before pushing onto the stack.
Otherwise, execution aborts, and verification fails.

### Supporting

`List n:u32 [u32]*n`
:   Consume a length $n$, and $n$ items from the input.
    Push a list containing those `u32`s to the stack.


### Variables and Symbols:

`EVar <u32>`
:   Push an `EVar` onto the stack.

`SVar <u32>`
:   Push a `SVar` onto the stack.

`Symbol <u32>`
:   Push a `Symbol` onto the stack.

`Implication`/`Application`/`Exists`/`Mu`
:   Consume the two patterns from the stack,
    and push an implication/application/exists/mu to the stack
    with appropriate arguments, performing well formedness checks as needed.

### Axiom Schemas

`Lukaseiwicz`/`Quantifier`/`PropagationOr`/`PropagationExists`/`PreFixpoint`/`Existance`/`Singleton`
:   Push proof term corresponding to axiom schema onto the stack.

### Meta inference

`MetaVar <u32>`
:   Consume the first five entries from the stack (corresponding to the
    meta-requirements), and push an `MetaVar` onto the stack.

`ESubst`/`SSubst`
:   Consume the one metavariable from the stack and use it to construct a substitution.

`Instantiate <metavar_id:u32>`
:   Consume a `Proof` and `Pattern` off the stack, and push the instantiated proof term to the stack,
    checking wellformedness as needed.

### Inference rules

`ModusPonens`/`Generalization`/`Frame`/`Substitution`/`KnasterTarski`
:   Consume one or two `Proof` terms off the stack and push the new proof term.

### Memory manipulation:

`Save i:u32`
:   Store the top of the stack to the specified index $i$ in memory.
    This overwrites existing data stored there.
    It is recommended to indices as compactly as possible.
    Otherwise, the performance of the checker may be affected.

`Load i:u32`
:   Push the `Term` at index $i$ to the top of the stack.

`Delete i:u32`
:   Remove the `Term` at index $i$ from memory. This is not strictly needed, but
    will allow the verifier to use less memory. The memory slot is not
    considered for reuse by the `Save` instruction.


### Journal manipulation.

`Publish`
:   * During the `gamma` phase, consume a pattern from the stack and push it to the list of axioms.
    * During the `claim` phase consume a pattern from the stack and push it to the queue of claims.
    * During the `proof` phase consume a proof from the stack
      and a claim from the queue of claims and assert that they are equal.


### Stack manipulation.

`Pop`
:   Consume the top of the stack.

### Technical

`EOF`
:   End the current phase.


Future considerations
=====================

-   De Brujin-like nameless representation: A nameless representation would
    simplify substitution and well-formedness checking.
-   Since we are constructing an on-the-wire format for machine use, there are
    other nameless representations we may consider. For example, we can consider
    two variables (element, set, meta-) equal, only if they are the identical
    DAG node. That is, the variable was constructed once, and retrieved using
    the `Load` command.




